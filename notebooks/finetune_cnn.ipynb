{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Richard\\Anaconda3\\envs\\pytorch-cpu\\lib\\site-packages\\librosa\\util\\decorators.py:9: NumbaDeprecationWarning: \u001b[1mAn import was requested from a module that has moved location.\n",
      "Import of 'jit' requested from: 'numba.decorators', please update to use 'numba.core.decorators' or pin to Numba version 0.48.0. This alias will not be present in Numba version 0.50.0.\u001b[0m\n",
      "  from numba.decorators import jit as optional_jit\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "import time\n",
    "import copy\n",
    "\n",
    "import numpy as np\n",
    "import random\n",
    "import pandas as pd\n",
    "import librosa\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "import torch\n",
    "torch.backends.cudnn.benchmark=True\n",
    "torch.manual_seed(0)\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.optim import lr_scheduler\n",
    "import torch.utils.data as utils\n",
    "\n",
    "sys.path.append('audio_tagging_functions')\n",
    "from models import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device: cpu\n"
     ]
    }
   ],
   "source": [
    "MODEL_TYPE = \"Transfer_Cnn6\"\n",
    "MODEL_PATH = \"pretrained_models/Cnn6_mAP=0.343.pth\"\n",
    "\n",
    "LABELS_PATH = \"labels/all_data.csv\"\n",
    "DATA_PATH = \"audiodata/wav\"\n",
    "\n",
    "# Audio parameters\n",
    "SR = 14000             # Sample Rate\n",
    "AUDIO_DURATION = 10    # 10 seconds duration window for all audios\n",
    "\n",
    "# Model parameters\n",
    "NB_SPECIES = 13        # Number of classes\n",
    "LR = 1e-3              # Learning Rate\n",
    "BATCH_SIZE = 16\n",
    "EPOCHS = 2\n",
    "\n",
    "# Misc parameters\n",
    "RANDOM_STATE = 42\n",
    "random.seed(RANDOM_STATE)\n",
    "\n",
    "DEVICE = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
    "print(f\"Device: {DEVICE}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Data and Initialize DataLoaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>fname</th>\n",
       "      <th>iD</th>\n",
       "      <th>length</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>audiodata/wav/sturnus_vulgaris/sturnus_vulgari...</td>\n",
       "      <td>548389</td>\n",
       "      <td>59</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>audiodata/wav/sturnus_vulgaris/sturnus_vulgari...</td>\n",
       "      <td>546936</td>\n",
       "      <td>32</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>audiodata/wav/sturnus_vulgaris/sturnus_vulgari...</td>\n",
       "      <td>546935</td>\n",
       "      <td>65</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>audiodata/wav/sturnus_vulgaris/sturnus_vulgari...</td>\n",
       "      <td>543800</td>\n",
       "      <td>36</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>audiodata/wav/sturnus_vulgaris/sturnus_vulgari...</td>\n",
       "      <td>542770</td>\n",
       "      <td>44</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               fname      iD  length  label\n",
       "0  audiodata/wav/sturnus_vulgaris/sturnus_vulgari...  548389      59      0\n",
       "1  audiodata/wav/sturnus_vulgaris/sturnus_vulgari...  546936      32      0\n",
       "2  audiodata/wav/sturnus_vulgaris/sturnus_vulgari...  546935      65      0\n",
       "3  audiodata/wav/sturnus_vulgaris/sturnus_vulgari...  543800      36      0\n",
       "4  audiodata/wav/sturnus_vulgaris/sturnus_vulgari...  542770      44      0"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels_df = pd.read_csv(LABELS_PATH)\n",
    "labels_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_df2array(df):\n",
    "    \"\"\"\n",
    "    Loads and stores arrays, and also returns a dict of missing files.\n",
    "    \"\"\"\n",
    "    nonValidDict = {}\n",
    "    waveforms_list = []\n",
    "    ConstantShape = SR * AUDIO_DURATION # Zero-padding\n",
    "    \n",
    "    for idx in range(len(df)):\n",
    "        filename = df['fname'][idx]\n",
    "        length = df['length'][idx]\n",
    "        label = df['label'][idx]\n",
    "\n",
    "        if os.path.isfile(filename):\n",
    "            waveform = load_waveform2numpy(filename, length, sample_rate=SR)\n",
    "            length_waveform = len(waveform)\n",
    "\n",
    "            # Zero-padding\n",
    "            if length_waveform != ConstantShape:\n",
    "                waveform = np.pad(waveform, (0, ConstantShape - length_waveform), 'constant')\n",
    "\n",
    "            waveforms_list.append([waveform, label])\n",
    "\n",
    "        else:\n",
    "            iD = df['iD'][idx]\n",
    "            nonValidDict[iD] = filename\n",
    "\n",
    "    return waveforms_list, nonValidDict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_waveform2numpy(filename, length, sample_rate):\n",
    "    \"\"\"\n",
    "    Loads from filename path and returns numpy array.\n",
    "    \"\"\"\n",
    "    # Random crop of a 10 sec segment\n",
    "    offset = random.randint(0, length-AUDIO_DURATION)\n",
    "\n",
    "    waveform, _ = librosa.core.load(filename, sr=sample_rate, mono=True, offset=offset, duration=AUDIO_DURATION)\n",
    "    #waveform = waveform[None, :]    # (1, audio_length)\n",
    "\n",
    "    return waveform"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Valid files: 6138\n",
      "Unvalid files: 9\n",
      "Wall time: 30.8 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "waveforms_list, nonValidDict = load_df2array(labels_df)\n",
    "print(f\"Valid files: {len(waveforms_list)}\\nUnvalid files: {len(nonValidDict)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_all:(6138, 140000), y_all:(6138,)\n"
     ]
    }
   ],
   "source": [
    "waveforms_arrays = [x[0] for x in waveforms_list]\n",
    "X_all = np.vstack(waveforms_arrays)\n",
    "y_all = np.array([x[1] for x in waveforms_list])\n",
    "print(f\"X_all:{X_all.shape}, y_all:{y_all.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Plot the distribution of labels\n",
    "plt.hist(y_all, bins=20)\n",
    "plt.title(\"Distribution of labels\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train:(4419, 140000), X_val:(1105, 140000), X_test:(614, 140000)\n",
      "y_train:(4419,), y_val:(1105,), y_test:(614,)\n"
     ]
    }
   ],
   "source": [
    "# Split Data in train, validation, test\n",
    "X_training, X_test, y_training, y_test = train_test_split(X_all, y_all, test_size=0.1, \n",
    "                                                          random_state=RANDOM_STATE, stratify=y_all)\n",
    "X_train, X_val, y_train, y_val = train_test_split(X_training, y_training, test_size=0.2, \n",
    "                                                  random_state=RANDOM_STATE, stratify=y_training)\n",
    "print(f\"X_train:{X_train.shape}, X_val:{X_val.shape}, X_test:{X_test.shape}\")\n",
    "print(f\"y_train:{y_train.shape}, y_val:{y_val.shape}, y_test:{y_test.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_dataloaders(x, y):\n",
    "    tensor_x = torch.from_numpy(x).float().to(DEVICE)\n",
    "    tensor_y = torch.from_numpy(y).long().to(DEVICE)\n",
    "\n",
    "    tensordataset = utils.TensorDataset(tensor_x, tensor_y)\n",
    "    dataloader = utils.DataLoader(tensordataset, batch_size=BATCH_SIZE)\n",
    "\n",
    "    return dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainloader = get_dataloaders(X_train, y_train)\n",
    "validationloader = get_dataloaders(X_val, y_val)\n",
    "testloader = get_dataloaders(X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load the pretrained model\n",
    "\n",
    "Inspired by: https://github.com/qiuqiangkong/audioset_tagging_cnn/blob/master/pytorch/finetune_template.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Transfer_Cnn6(nn.Module):\n",
    "    def __init__(self, sample_rate, window_size, hop_size, mel_bins, fmin, \n",
    "        fmax, classes_num, freeze_base):\n",
    "        \"\"\"Classifier for a new task using pretrained Cnn14 as a sub module.\n",
    "        \"\"\"\n",
    "        super(Transfer_Cnn6, self).__init__()\n",
    "        audioset_classes_num = 527\n",
    "        \n",
    "        self.base = Cnn6(sample_rate, window_size, hop_size, mel_bins, fmin, \n",
    "            fmax, audioset_classes_num)\n",
    "\n",
    "        # Transfer to another task layer\n",
    "        self.fc_transfer = nn.Linear(512, classes_num, bias=True)\n",
    "\n",
    "        if freeze_base:\n",
    "            # Freeze AudioSet pretrained layers\n",
    "            for param in self.base.parameters():\n",
    "                param.requires_grad = False\n",
    "\n",
    "        self.init_weights()\n",
    "\n",
    "    def init_weights(self):\n",
    "        init_layer(self.fc_transfer)\n",
    "\n",
    "    def load_from_pretrain(self, pretrained_checkpoint_path):\n",
    "        checkpoint = torch.load(pretrained_checkpoint_path, map_location=torch.device(DEVICE))\n",
    "        self.base.load_state_dict(checkpoint['model'])\n",
    "\n",
    "    def forward(self, input, mixup_lambda=None):\n",
    "        \"\"\"Input: (batch_size, data_length)\n",
    "        \"\"\"\n",
    "        output_dict = self.base(input, mixup_lambda)\n",
    "        embedding = output_dict['embedding']\n",
    "\n",
    "        clipwise_output =  torch.log_softmax(self.fc_transfer(embedding), dim=-1)\n",
    "        output_dict['clipwise_output'] = clipwise_output\n",
    " \n",
    "        return output_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Richard\\Anaconda3\\envs\\pytorch-cpu\\lib\\site-packages\\librosa\\filters.py:235: UserWarning: Empty filters detected in mel frequency basis. Some channels will produce empty responses. Try increasing your sampling rate (and fmax) or reducing n_mels.\n",
      "  warnings.warn('Empty filters detected in mel frequency basis. '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Transfer_Cnn6(\n",
      "  (base): Cnn6(\n",
      "    (spectrogram_extractor): Spectrogram(\n",
      "      (stft): STFT(\n",
      "        (conv_real): Conv1d(1, 513, kernel_size=(1024,), stride=(320,), bias=False)\n",
      "        (conv_imag): Conv1d(1, 513, kernel_size=(1024,), stride=(320,), bias=False)\n",
      "      )\n",
      "    )\n",
      "    (logmel_extractor): LogmelFilterBank()\n",
      "    (spec_augmenter): SpecAugmentation(\n",
      "      (time_dropper): DropStripes()\n",
      "      (freq_dropper): DropStripes()\n",
      "    )\n",
      "    (bn0): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (conv_block1): ConvBlock5x5(\n",
      "      (conv1): Conv2d(1, 64, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), bias=False)\n",
      "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "    (conv_block2): ConvBlock5x5(\n",
      "      (conv1): Conv2d(64, 128, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), bias=False)\n",
      "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "    (conv_block3): ConvBlock5x5(\n",
      "      (conv1): Conv2d(128, 256, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), bias=False)\n",
      "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "    (conv_block4): ConvBlock5x5(\n",
      "      (conv1): Conv2d(256, 512, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), bias=False)\n",
      "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "    (fc1): Linear(in_features=512, out_features=512, bias=True)\n",
      "    (fc_audioset): Linear(in_features=512, out_features=527, bias=True)\n",
      "  )\n",
      "  (fc_transfer): Linear(in_features=512, out_features=13, bias=True)\n",
      ")\n",
      "GPU number: 0\n",
      "Load pretrained model successfully!\n"
     ]
    }
   ],
   "source": [
    "# Model Audio parameters\n",
    "window_size = 1024\n",
    "hop_size = 320\n",
    "mel_bins = 64\n",
    "fmin = 50\n",
    "fmax = 14000\n",
    "freeze_base = True\n",
    "sample_rate = 14000\n",
    "classes_num = NB_SPECIES\n",
    "\n",
    "# Initialize Model\n",
    "Model = eval(MODEL_TYPE)\n",
    "model = Model(sample_rate, window_size, hop_size, mel_bins, fmin, fmax, \n",
    "    classes_num, freeze_base)\n",
    "\n",
    "print(model)\n",
    "\n",
    "# Load pretrained model\n",
    "model.load_from_pretrain(MODEL_PATH)\n",
    "\n",
    "# Parallel\n",
    "print('GPU number: {}'.format(torch.cuda.device_count()))\n",
    "model = torch.nn.DataParallel(model)\n",
    "\n",
    "if DEVICE == 'cuda':\n",
    "    model.to(DEVICE)\n",
    "\n",
    "print('Load pretrained model successfully!')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training Part\n",
    "\n",
    "https://pytorch.org/tutorials/beginner/transfer_learning_tutorial.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataloaders = {\"train\": trainloader,\n",
    "               \"val\": validationloader}\n",
    "dataset_sizes = {\"train\": len(trainloader)*BATCH_SIZE,\n",
    "                 \"val\": len(validationloader)*BATCH_SIZE}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model, criterion, optimizer, scheduler, dataloaders, num_epochs=25):\n",
    "    since = time.time()\n",
    "\n",
    "    best_model_wts = copy.deepcopy(model.state_dict())\n",
    "    best_acc = 0.0\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        lasttime = time.time()\n",
    "        print('Epoch {}/{}'.format(epoch, num_epochs - 1))\n",
    "        print('-' * 10)\n",
    "\n",
    "        # Each epoch has a training and validation phase\n",
    "        for phase in ['train', 'val']:\n",
    "            if phase == 'train':\n",
    "                model.train()  # Set model to training mode\n",
    "            else:\n",
    "                model.eval()   # Set model to evaluate mode\n",
    "\n",
    "            running_loss = 0.0\n",
    "            running_corrects = 0\n",
    "\n",
    "            # Iterate over data.\n",
    "            for inputs, labels in dataloaders[phase]:\n",
    "                inputs = inputs.to(DEVICE)\n",
    "                labels = labels.to(DEVICE)\n",
    "\n",
    "                # zero the parameter gradients\n",
    "                optimizer.zero_grad()\n",
    "\n",
    "                # forward\n",
    "                # track history if only in train\n",
    "                with torch.set_grad_enabled(phase == 'train'):\n",
    "                    outputs = model(inputs)['clipwise_output']\n",
    "                    _, preds = torch.max(outputs, 1)\n",
    "                    loss = criterion(outputs, labels)\n",
    "\n",
    "                    # backward + optimize only if in training phase\n",
    "                    if phase == 'train':\n",
    "                        loss.backward()\n",
    "                        optimizer.step()\n",
    "\n",
    "                # statistics\n",
    "                running_loss += loss.item() * inputs.size(0)\n",
    "                running_corrects += torch.sum(preds == labels.data)\n",
    "            if phase == 'train':\n",
    "                scheduler.step()\n",
    "\n",
    "            epoch_loss = running_loss / dataset_sizes[phase]\n",
    "            epoch_acc = running_corrects.double() / dataset_sizes[phase]\n",
    "\n",
    "            print('{} Loss: {:.4f} Acc: {:.4f}'.format(\n",
    "                phase, epoch_loss, epoch_acc))\n",
    "\n",
    "            # deep copy the model\n",
    "            if phase == 'val' and epoch_acc > best_acc:\n",
    "                best_acc = epoch_acc\n",
    "                best_model_wts = copy.deepcopy(model.state_dict())\n",
    "\n",
    "        print(f\">Epoch's duration {time.time() - lasttime} seconds<\\n\")\n",
    "\n",
    "    time_elapsed = time.time() - since\n",
    "    print('Training complete in {:.0f}m {:.0f}s'.format(\n",
    "        time_elapsed // 60, time_elapsed % 60))\n",
    "    print('Best val Acc: {:4f}'.format(best_acc))\n",
    "\n",
    "    # load best model weights\n",
    "    model.load_state_dict(best_model_wts)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.SGD(model.parameters(), lr=LR, momentum=0.9)\n",
    "\n",
    "# Decay LR by a factor of 0.1 every 7 epochs\n",
    "exp_lr_scheduler = lr_scheduler.StepLR(optimizer, step_size=7, gamma=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0/1\n",
      "----------\n",
      "train Loss: 2.4345 Acc: 0.1934\n",
      "val Loss: 2.0939 Acc: 0.3473\n",
      ">Epoch's duration 1372.8128345012665 seconds<\n",
      "\n",
      "Epoch 1/1\n",
      "----------\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<timed exec>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-15-a8bb252811e0>\u001b[0m in \u001b[0;36mtrain_model\u001b[1;34m(model, criterion, optimizer, scheduler, dataloaders, num_epochs)\u001b[0m\n\u001b[0;32m     31\u001b[0m                 \u001b[1;31m# track history if only in train\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     32\u001b[0m                 \u001b[1;32mwith\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mset_grad_enabled\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mphase\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m'train'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 33\u001b[1;33m                     \u001b[0moutputs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'clipwise_output'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     34\u001b[0m                     \u001b[0m_\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpreds\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmax\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0moutputs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     35\u001b[0m                     \u001b[0mloss\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0moutputs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\pytorch-cpu\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m    530\u001b[0m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    531\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 532\u001b[1;33m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    533\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    534\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\pytorch-cpu\\lib\\site-packages\\torch\\nn\\parallel\\data_parallel.py\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, *inputs, **kwargs)\u001b[0m\n\u001b[0;32m    138\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    139\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdevice_ids\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 140\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmodule\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    141\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    142\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mt\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mchain\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmodule\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mparameters\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmodule\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbuffers\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\pytorch-cpu\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m    530\u001b[0m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    531\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 532\u001b[1;33m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    533\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    534\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-12-7770309e1bfc>\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, input, mixup_lambda)\u001b[0m\n\u001b[0;32m     30\u001b[0m         \"\"\"Input: (batch_size, data_length)\n\u001b[0;32m     31\u001b[0m         \"\"\"\n\u001b[1;32m---> 32\u001b[1;33m         \u001b[0moutput_dict\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbase\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmixup_lambda\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     33\u001b[0m         \u001b[0membedding\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0moutput_dict\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'embedding'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     34\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\pytorch-cpu\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m    530\u001b[0m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    531\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 532\u001b[1;33m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    533\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    534\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Projets\\Silentcities2\\audio_tagging_functions\\models.py\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, input, mixup_lambda)\u001b[0m\n\u001b[0;32m    470\u001b[0m         \u001b[0mx\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mconv_block1\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpool_size\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m2\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpool_type\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'avg'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    471\u001b[0m         \u001b[0mx\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mF\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdropout\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mp\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0.2\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtraining\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtraining\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 472\u001b[1;33m         \u001b[0mx\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mconv_block2\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpool_size\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m2\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpool_type\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'avg'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    473\u001b[0m         \u001b[0mx\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mF\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdropout\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mp\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0.2\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtraining\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtraining\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    474\u001b[0m         \u001b[0mx\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mconv_block3\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpool_size\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m2\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpool_type\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'avg'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\pytorch-cpu\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m    530\u001b[0m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    531\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 532\u001b[1;33m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    533\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    534\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Projets\\Silentcities2\\audio_tagging_functions\\models.py\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, input, pool_size, pool_type)\u001b[0m\n\u001b[0;32m    100\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    101\u001b[0m         \u001b[0mx\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 102\u001b[1;33m         \u001b[0mx\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mF\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrelu_\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbn1\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mconv1\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    103\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mpool_type\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m'max'\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    104\u001b[0m             \u001b[0mx\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mF\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmax_pool2d\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkernel_size\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mpool_size\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "%%time\n",
    "trained_model = train_model(model, criterion, optimizer, \n",
    "                            exp_lr_scheduler, dataloaders, num_epochs=EPOCHS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
